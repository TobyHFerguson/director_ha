roleconfigs {
  ZOOKEEPER {
    SERVER {
      dataDir: "/data0/zookeeper"
      dataLogDir: "/data0/zookeeper"
      maxClientCnxns: 600
      maxSessionTimeout: 60000
    }
  }
  HDFS {
    NAMENODE {
      dfs_namenode_handler_count: 60
      dfs_namenode_service_handler_count: 60
      dfs_federation_namenode_nameservice: ${HDFS_NAMESERVICE}
      autofailover_enabled: true
      dfs_namenode_quorum_journal_name: ${HDFS_NAMESERVICE}
      dfs_name_dir_list: "/data2/hdfs/nn"
      namenode_java_heapsize: 4294967296
    }
    JOURNALNODE {
      dfs_journalnode_edits_dir: "/data1/hdfs/jn"
    }
    DATANODE {
      dfs_datanode_port: 50010
      dfs_datanode_http_port: 50075
      dfs_data_dir_list: "/data0/dfs/dn,/data1/dfs/dn,/data2/dfs/dn,/data3/dfs/dn"
      dfs_datanode_du_reserved: 10737418240

    }
    HTTPFS {
      httpfs_load_balancer: ${LOAD_BALANCER}":14000"
    }
  }
  YARN {
    RESOURCEMANAGER {
      resourcemanager_fair_scheduler_assign_multiple: false
      resourcemanager_fair_scheduler_preemption: true
      yarn_scheduler_fair_continuous_scheduling_enabled: true
      yarn_scheduler_maximum_allocation_mb: 16384
      yarn_scheduler_maximum_allocation_vcores: 2
    }
    GATEWAY {
      mapreduce_client_java_heapsize: 1073741824
      mapreduce_map_java_opts_max_heap: 886046720
      mapreduce_map_memory_mb: 1024
      mapreduce_reduce_java_opts_max_heap: 1771674010
      mapreduce_reduce_memory_mb: 2048
      mapreduce_client_config_safety_valve: "<property><name>mapreduce.job.encrypted-intermediate-data</name><value>true</value></property><property><name>mapreduce.job.encrypted-intermediate-data-key-size-bits</name><value>256</value></property>"
      mapreduce_job_acl_modify_job: hue ${ADMIN_GROUP}
      mapreduce_job_acl_view_job: hue ${ADMIN_GROUP}
    }
    NODEMANAGER {
      yarn_nodemanager_resource_cpu_vcores: 2
      yarn_nodemanager_resource_memory_mb: 16384
      container_executor_min_user_id: 100
      yarn_nodemanager_local_dirs: "/data0/yarn/nm,/data1/yarn/nm,/data2/yarn/nm,/data3/yarn/nm"
      yarn_nodemanager_log_dirs: "/data0/yarn/container-logs,/data1/yarn/container-logs,/data2/yarn/container-logs,/data3/yarn/container-logs"
    }
    JOBHISTORY {
     mapreduce_cluster_acls_enabled: true
    }
  }
  HIVE {
    HIVESERVER2 {
      hiveserver2_enable_impersonation: false
      hiveserver2_java_heapsize: 6442450944
      hiveserver2_enable_mapjoin: false
      hiveserver2_max_threads: 200
      hiveserver2_spark_driver_memory: 11596411699
      hiveserver2_spark_executor_cores: 5
      hiveserver2_spark_executor_memory: 6845104128
      hiveserver2_spark_yarn_driver_memory_overhead: 1228
      hiveserver2_spark_yarn_executor_memory_overhead: 1152
      process_auto_restart: true
      hiverserver2_load_balancer: ${LOAD_BALANCER}":10000"
    }
    HIVEMETASTORE {
        hive_enable_db_notification: true
        hive_metastore_delegation_token_store: org.apache.hadoop.hive.thrift.DBTokenStore
    }
  }
  SPARK_ON_YARN {
    GATEWAY {
      spark_io_encryption_enabled: true
      spark_network_encryption_enabled: true
      "spark-conf/spark-defaults.conf_client_config_safety_valve": "spark.shuffle.encryption.keySizeBits=256\nspark.network.sasl.serverAlwaysEncrypt=true"
    }
    SPARK_YARN_HISTORY_SERVER {
      history_server_spnego_enabled: true
      history_server_max_heapsize: 4294967296
    }
  }
  SPARK2_ON_YARN {
    GATEWAY {
      spark_io_encryption_enabled: true
      spark_network_encryption_enabled: true
      "spark2-conf/spark-defaults.conf_client_config_safety_valve": "spark.authenticate.enableSaslEncryption=true\nspark.network.crypto.enabled=true\nspark.io.encryption.enabled\nspark.io.encryption.keySizeBits=256\nspark.network.sasl.serverAlwaysEncrypt=true"
      "spark2-conf/spark-env.sh_client_config_safety_valve": """if [ -z "${PYSPARK_PYTHON}" ]; then export PYSPARK_PYTHON=/opt/cloudera/parcels/Anaconda/bin/python; fi"""
    }
    SPARK2_YARN_HISTORY_SERVER {
      history_server_spnego_enabled: true
      history_server_max_heapsize: 4294967296
    }
  }
  HUE {
    HUE_SERVER {
      hue_server_hive_safety_valve: "<property><name>hive.server2.authentication</name><value>kerberos</value></property>"
    }
  }
  OOZIE {
    OOZIE_SERVER {
      oozie_config_safety_valve: "<property><name>oozie.action.launcher.mapreduce.job.ubertask.enable</name><value>false</value></property>"
      oozie_plugins_list: "org.apache.oozie.service.ZKLocksService,org.apache.oozie.service.ZKXLogStreamingService,org.apache.oozie.service.ZKJobsConcurrencyService,org.apache.oozie.service.ZKUUIDService"
    }
  }
}

# Cluster description
cluster {
    products {
        CDH: 5
        SPARK2: 2
    }

    parcelRepositories: [
        "http://archive.cloudera.com/cdh5/parcels/5.15/",
        "http://archive.cloudera.com/spark2/parcels/2/"
    ]


    services: [HDFS, YARN, ZOOKEEPER, HUE, SPARK_ON_YARN, HIVE, SPARK2_ON_YARN, OOZIE, SQOOP_CLIENT]

   configs {
        HDFS {
          dfs_ha_fencing_methods: "shell(true)"
          dfs_image_transfer_timeout: 120000
          dfs_namenode_acls_enabled: true
          dfs_permissions_supergroup: ${ADMIN_GROUP}
          dfs_umaskmode: "026"
          hadoop_secure_web_ui: true
          hadoop_security_authorization: true
          hadoop_rpc_protection: privacy
          dfs_data_transfer_protection: privacy
          dfs_namenode_acls_enabled: true
          hdfs_sentry_sync_enable: true
          dfs_encrypt_data_transfer: true
          dfs_encrypt_data_transfer_algorithm: "AES/CTR/NoPadding"
          hadoop_authorized_admin_users: "hdfs,mapred,yarn"
          hadoop_authorized_admin_groups: ${ADMIN_GROUP}
          hadoop_authorized_users: "flume,hdfs,solr,zookeeper,llama,httpfs,mapred,sqoop,yarn,kms,hive,sqoop2,oozie,hbase,sentry,impala,spark,hue"
          hadoop_authorized_groups: ${USER_GROUP}","${ADMIN_GROUP}
          core_site_safety_valve: "<property> \n<name>hadoop.security.key.default.bitlength</name> \n<value>256</value>\n</property>"
          hdfs_sentry_sync_path_prefixes: "/user/hive/warehouse,/data"
          extra_auth_to_local_rules: "RULE:[1:$1](.*)s/(.*)/$1/L"
        }
    
        YARN {
          hadoop_secure_web_ui: true
          yarn_admin_acl: yarn","${ADMIN_GROUP}
        }
    
        ZOOKEEPER {
          zookeeper_canary_connection_timeout: 30000
          enableSecurity: true
          quorum_auth_enable_sasl: true
        }

        SPARK_ON_YARN {
          spark_authenticate: true
        }

        SPARK2_ON_YARN {
          spark_authenticate: true
        }
        HUE {
          auth_backend: "desktop.auth.backend.LdapBackend"
          base_dn: "DC=trng201809,DC=lab"
          bind_dn: ${LDAP_BIND_DN}"@"${REALM}
          bind_password: ${LDAP_BIND_PW}
          group_filter: "(&(objectclass=group)(|(sAMAccountName=cloudera_users)(samAccountName=cloudera_admins)))"
          group_member_attr: member
          group_name_attr: cn
          hue_service_safety_valve: "[desktop]\n[[ldap]]\nsync_groups_on_login=true\nsubgroups=nested\nnested_members_search_depth=10\n[[session]]\nttl=86400\nsecure=true\nexpire_at_browser_close=true\n[impala]\nclose_queries=true\n[beeswax]\nclose_queries=true"
          ldap_url: ${LDAP_URL}
          nt_domain: ""
          search_bind_authentication: true
          time_zone: "America/New_York"
          use_start_tls: false
          user_filter: "(&(objectClass=user)(|(memberOf=CN=cloudera_users,OU=groups,OU=trng,OU=cloudera,DC=trng201809,DC=lab)(memberOf=CN=cloudera_admins,OU=groups,OU=trng,OU=cloudera,DC=trng201809,DC=lab)))"
          user_name_attr: samAccountName
          ldap_cert: "/opt/cloudera/security/pki/ca-certs.pem"
        }
        HIVE {
          hive_proxy_user_groups_list: "hue,impala,hive,sentry,"${ADMIN_GROUP}","${USER_GROUP}
          hiveserver2_enable_ldap_auth: true
          hiveserver2_ldap_uri: ${LDAP_URL}
        }
        OOZIE {
        #  oozie_load_balancer: ${LOAD_BALANCER}":11443"
          oozie_load_balancer: ${LOAD_BALANCER}
          oozie_load_balancer_http_port: 11000
          oozie_load_balancer_https_port: 11443
        }
   }
databaseTemplates: {
    HIVE {
      name: hivet
      databaseServerName: rds-mysql-prod1 
      databaseNamePrefix: hive
      usernamePrefix: hiveu
    },

    HUE {
      name: huet
      databaseServerName: rds-mysql-prod1 
      databaseNamePrefix: hue
      usernamePrefix: hueu
    },

    OOZIE {
      name: ooziet
      databaseServerName: rds-mysql-prod1 
      databaseNamePrefix: oozie
      usernamePrefix: oozieu
    }

  }

#####################
    master {
        count: 2

        instance: ${instances.master} {
            tags {
                group: masters
            }
            dataDiskCount: 3
            ebsVolumeCount: 3
            ebsVolumeType: gp2
        }

        roles {
            HDFS: [NAMENODE, FAILOVERCONTROLLER, JOURNALNODE]
            ZOOKEEPER: [SERVER]
            YARN: [RESOURCEMANAGER]
            HIVE: [HIVEMETASTORE]
        }

        configs {
          ZOOKEEPER {
            SERVER: ${roleconfigs.ZOOKEEPER.SERVER}
          }
          HDFS {
            NAMENODE:  ${roleconfigs.HDFS.NAMENODE}
            JOURNALNODE: ${roleconfigs.HDFS.JOURNALNODE}
          }
          YARN {
            RESOURCEMANAGER: ${roleconfigs.YARN.RESOURCEMANAGER}
          }
          HIVE {
            HIVEMETASTORE: ${roleconfigs.HIVE.HIVEMETASTORE}
          }
        }
    }


    util-1 {
        count: 1
        instance: ${instances.util} {
            tags {
                group: util-1
            }
            dataDiskCount: 2
            ebsVolumeCount: 2
            ebsVolumeType: gp2
        }

        roles {
            HDFS: [JOURNALNODE, HTTPFS]
            ZOOKEEPER: [SERVER]
            HIVE: [HIVESERVER2]
            SPARK2_ON_YARN: [SPARK2_YARN_HISTORY_SERVER, GATEWAY]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER, GATEWAY]
            OOZIE: [OOZIE_SERVER]
            HUE: [HUE_SERVER, HUE_LOAD_BALANCER]
        }
        configs {
          HDFS {
            JOURNALNODE: ${roleconfigs.HDFS.JOURNALNODE}
            HTTPFS: ${roleconfigs.HDFS.HTTPFS}
          }
          ZOOKEEPER {
            SERVER: ${roleconfigs.ZOOKEEPER.SERVER}
          }
          HIVE {
            HIVESERVER2: ${roleconfigs.HIVE.HIVESERVER2}
          }
          SPARK_ON_YARN {
            SPARK_YARN_HISTORY_SERVER:  ${roleconfigs.SPARK_ON_YARN.SPARK_YARN_HISTORY_SERVER}
            GATEWAY: ${roleconfigs.SPARK_ON_YARN.GATEWAY}
          }
          SPARK2_ON_YARN {
            SPARK2_YARN_HISTORY_SERVER:  ${roleconfigs.SPARK2_ON_YARN.SPARK2_YARN_HISTORY_SERVER}
            GATEWAY: ${roleconfigs.SPARK2_ON_YARN.GATEWAY}
          }
          HUE {
            HUE_SERVER: ${roleconfigs.HUE.HUE_SERVER}
          }
          OOZIE {
            OOZIE_SERVER: ${roleconfigs.OOZIE.OOZIE_SERVER}
          }
        }
    }

    util-2 {
        count: 1

        instance: ${instances.util} {
            tags {
                group: util-2
            }
            dataDiskCount: 2
            ebsVolumeCount: 2
            ebsVolumeType: gp2
        }

        roles {
            HDFS: [HTTPFS]
            HIVE: [HIVESERVER2]
            YARN: [JOBHISTORY]
            OOZIE: [OOZIE_SERVER]
            HUE: [HUE_SERVER, HUE_LOAD_BALANCER]
            SPARK2_ON_YARN: [GATEWAY]
            SPARK_ON_YARN: [GATEWAY]
        }

        configs {
          HIVE {
            HIVESERVER2: ${roleconfigs.HIVE.HIVESERVER2}
          }
          HUE {
            HUE_SERVER: ${roleconfigs.HUE.HUE_SERVER}
          }
          SPARK_ON_YARN {
            GATEWAY: ${roleconfigs.SPARK_ON_YARN.GATEWAY}
          }
          SPARK2_ON_YARN {
            GATEWAY: ${roleconfigs.SPARK2_ON_YARN.GATEWAY}
          }
          OOZIE {
            OOZIE_SERVER: ${roleconfigs.OOZIE.OOZIE_SERVER}
          }
          HDFS {
            HTTPFS: ${roleconfigs.HDFS.HTTPFS}
          }
          YARN {
            JOBHISTORY: ${roleconfigs.YARN.JOBHISTORY}
          }
        }
    }
    gateway {
        count: 1
        instance: ${instances.edge} {
            tags {
                group: gateway
            }
            dataDiskCount: 1
            dataDiskSizeGb: 500
            ebsVolumeCount: 1
            ebsVolumeType: st1
        }

        roles {
            YARN: [GATEWAY]
            HIVE: [GATEWAY]
            SPARK_ON_YARN: [GATEWAY]
            SPARK2_ON_YARN: [GATEWAY]
            SQOOP_CLIENT: [GATEWAY]
        }
        configs {
          YARN {
            GATEWAY: ${roleconfigs.YARN.GATEWAY}
          }
          SPARK_ON_YARN {
            GATEWAY: ${roleconfigs.SPARK_ON_YARN.GATEWAY}
          }
          SPARK2_ON_YARN {
            GATEWAY: ${roleconfigs.SPARK2_ON_YARN.GATEWAY}
          }
      }
    }

    workers {
        count: 3
        minCount: 3
        instance: ${instances.worker} {
            tags {
                group: worker
            }
            dataDiskCount: 4
            dataDiskSizeGb: 500
            ebsVolumeCount: 4
            ebsVolumeType: st1 
        }

        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            SPARK_ON_YARN: [GATEWAY]
            SPARK2_ON_YARN: [GATEWAY]
        }
        configs {
          HDFS {
            DATANODE: ${roleconfigs.HDFS.DATANODE}
          }
          YARN {
            NODEMANAGER: ${roleconfigs.YARN.NODEMANAGER}
          }
          SPARK_ON_YARN {
            GATEWAY: ${roleconfigs.SPARK_ON_YARN.GATEWAY}
          }
          SPARK2_ON_YARN {
            GATEWAY: ${roleconfigs.SPARK2_ON_YARN.GATEWAY}
          }
        }
    }

    postCreateScripts: ["""#! /usr/bin/env bash
# Setup Navigator Admin group, change auth_backend_order to EXTERNAL_ONLY and restart Navigator
url_escaped_group_dn=$( echo -n "CN=cloudera_admins,OU=groups,OU=trng,OU=cloudera,DC=trng201809,DC=lab" | python -c 'import urllib; import sys; sys.stdout.write(urllib.quote(sys.stdin.read()))'  )
navigator_url="https://$(echo ${DEPLOYMENT_HOST_PORT} | cut -d ':' -f 1):7187/api/v11/auth/group/${url_escaped_group_dn}?groupName=cloudera_admins"

/usr/bin/curl --noproxy "*" --insecure -su ${CM_USERNAME}:${CM_PASSWORD} -i -H "content-type:application/json" -d "[7]" -X PUT ${navigator_url}

NAV_CONFIG_GROUP=$( /usr/bin/curl --noproxy "*" --insecure -su ${CM_USERNAME}:${CM_PASSWORD} -X GET "https://${DEPLOYMENT_HOST_PORT}/api/v19/cm/service/roleConfigGroups" | python -c 'import json, sys; j=json.loads(sys.stdin.read()); print (" ".join([i["name"] for i in j["items"] if i["roleType"] == "NAVIGATORMETASERVER"]))')

/usr/bin/curl --noproxy "*" --insecure -su ${CM_USERNAME}:${CM_PASSWORD} -i -H "content-type:application/json" \
    -d "{\"items\":[{\"name\":\"auth_backend_order\",\"value\":\"EXTERNAL_ONLY\"}]}" \
    -X PUT "https://${DEPLOYMENT_HOST_PORT}/api/v19/cm/service/roleConfigGroups/${NAV_CONFIG_GROUP}/config"

NAV_ROLE=$( /usr/bin/curl --noproxy "*" --insecure -su ${CM_USERNAME}:${CM_PASSWORD} -X GET "https://${DEPLOYMENT_HOST_PORT}/api/v19/cm/service/roles" | python -c 'import json, sys; j=json.loads(sys.stdin.read()); print (" ".join([i["name"] for i in j["items"] if i["type"] == "NAVIGATORMETASERVER"]))')

/usr/bin/curl --noproxy "*" --insecure -su ${CM_USERNAME}:${CM_PASSWORD} -i -H "content-type:application/json" \
    -d "{\"items\":[\"${NAV_ROLE}\"]}" \
    -X POST "https://${DEPLOYMENT_HOST_PORT}/api/v19/cm/service/roleCommands/restart"

exit 0
    """
    ]
    preTerminateScriptsPaths: ["scripts/cleanup-cluster.sh"]
    preTerminateScripts: ["""#! /usr/bin/env bash

exit 0
    """
    ]
instancePostCreateScripts: ["""#!/bin/sh

rm -f /sbin/yum

"""${JAVA_HOME}"""/bin/keytool -importcert -noprompt -keystore """${JAVA_HOME}"""/jre/lib/security/jssecacerts -alias ClusterCA -storepass changeit -file /var/lib/cloudera-scm-agent/agent-cert/cm-auto-in_cluster_ca_cert.pem 

exit 0

    """]
}
