roleconfigs {
    ZOOKEEPER {
	SERVER {
	    dataDir: "/data0/zookeeper"
	    dataLogDir: "/data0/zookeeper"
	    maxClientCnxns: 600
	    maxSessionTimeout: 60000
	}
    }
    HDFS {
	NAMENODE {
	    dfs_namenode_handler_count: 60
	    dfs_namenode_service_handler_count: 60
	    dfs_federation_namenode_nameservice: ${HDFS_NAMESERVICE}
	    autofailover_enabled: true
	    dfs_namenode_quorum_journal_name: ${HDFS_NAMESERVICE}
	    dfs_name_dir_list: "/data2/hdfs/nn"
	    namenode_java_heapsize: 4294967296
	}
	JOURNALNODE {
	    dfs_journalnode_edits_dir: "/data1/hdfs/jn"
	}
	DATANODE {
	    dfs_datanode_port: 50010
	    dfs_datanode_http_port: 50075
	    dfs_data_dir_list: "/data0/dfs/dn,/data1/dfs/dn,/data2/dfs/dn,/data3/dfs/dn"
	    dfs_datanode_du_reserved: 10737418240
	}
    }
    YARN {
	RESOURCEMANAGER {
	    resourcemanager_fair_scheduler_assign_multiple: false
	    resourcemanager_fair_scheduler_preemption: true
	    yarn_scheduler_fair_continuous_scheduling_enabled: true
	    yarn_scheduler_maximum_allocation_mb: 16384
	    yarn_scheduler_maximum_allocation_vcores: 2
	}
	GATEWAY {
	    mapreduce_client_java_heapsize: 1073741824
	    mapreduce_map_java_opts_max_heap: 886046720
	    mapreduce_map_memory_mb: 1024
	    mapreduce_reduce_java_opts_max_heap: 1771674010
	    mapreduce_reduce_memory_mb: 2048
	    mapreduce_client_config_safety_valve: "<property><name>mapreduce.job.encrypted-intermediate-data</name><value>true</value></property><property><name>mapreduce.job.encrypted-intermediate-data-key-size-bits</name><value>256</value></property>"
	}
	NODEMANAGER {
	    yarn_nodemanager_resource_cpu_vcores: 2
	    yarn_nodemanager_resource_memory_mb: 16384
	    container_executor_min_user_id: 100
	    yarn_nodemanager_local_dirs: "/data0/yarn/nm,/data1/yarn/nm,/data2/yarn/nm,/data3/yarn/nm"
	    yarn_nodemanager_log_dirs: "/data0/yarn/container-logs,/data1/yarn/container-logs,/data2/yarn/container-logs,/data3/yarn/container-logs"
	}
    }
    HIVE {
	HIVESERVER2 {
	    hiveserver2_enable_impersonation: false
	    hiveserver2_java_heapsize: 6442450944
	    hiveserver2_enable_mapjoin: false
	    hiveserver2_max_threads: 200
	    process_auto_restart: true
	    hiverserver2_load_balancer: ${LOAD_BALANCER}":"${HIVE_SERVER_LOAD_BALANCER_PORT}
	}
	HIVEMETASTORE {
            hive_enable_db_notification: true
            hive_metastore_delegation_token_store: org.apache.hadoop.hive.thrift.DBTokenStore
	}
    }
}

# Cluster description
cluster {
    products {
        CDH: 5
    }

    parcelRepositories: [
        "http://archive.cloudera.com/cdh5/parcels/5.15/"
    ]


    services: [HDFS, YARN, ZOOKEEPER, HIVE]

    configs {
        HDFS {
            dfs_ha_fencing_methods: "shell(true)"
            dfs_image_transfer_timeout: 120000
            dfs_umaskmode: "026"
        }
	
	
        ZOOKEEPER {
            zookeeper_canary_connection_timeout: 30000
        }

    }
    databaseTemplates: {
	HIVE {
	    name: hivet
	    databaseServerName: rds-mysql-prod1 
	    databaseNamePrefix: hive
	    usernamePrefix: hiveu
	}
    }

#####################
    mastera {
	instance: ${instances.master} {
	    tags {
		group: masters
	    }
	    dataDiskCount: 3
	    ebsVolumeCount: 3
	    ebsVolumeType: gp2
	}

	roles {
	    HDFS: [NAMENODE, FAILOVERCONTROLLER, JOURNALNODE]
	    ZOOKEEPER: [SERVER]
	    YARN: [RESOURCEMANAGER]
	    HIVE: [HIVESERVER2, HIVEMETASTORE]
	}

	configs {
	    ZOOKEEPER {
		SERVER: ${roleconfigs.ZOOKEEPER.SERVER}
	    }
	    HDFS {
		NAMENODE:  ${roleconfigs.HDFS.NAMENODE}
		JOURNALNODE: ${roleconfigs.HDFS.JOURNALNODE}
	    }
	    YARN {
		RESOURCEMANAGER: ${roleconfigs.YARN.RESOURCEMANAGER}
	    }
	    HIVE {
		HIVEMETASTORE: ${roleconfigs.HIVE.HIVEMETASTORE}
		HIVESERVER2: ${roleconfigs.HIVE.HIVESERVER2}

	    }
	}
	include file("services/master.conf"), include file("services/aza.conf")
        count: 1
	instance.placementGroup: ${SPREAD_PLACEMENT_GROUP}
    }
    masterb { 
	instance: ${instances.master} {
	    tags {
		group: masters
	    }
	    dataDiskCount: 3
	    ebsVolumeCount: 3
	    ebsVolumeType: gp2
	}

	roles {
	    HDFS: [NAMENODE, FAILOVERCONTROLLER, JOURNALNODE]
	    ZOOKEEPER: [SERVER]
	    YARN: [RESOURCEMANAGER]
	    HIVE: [HIVESERVER2, HIVEMETASTORE]
	}

	configs {
	    ZOOKEEPER {
		SERVER: ${roleconfigs.ZOOKEEPER.SERVER}
	    }
	    HDFS {
		NAMENODE:  ${roleconfigs.HDFS.NAMENODE}
		JOURNALNODE: ${roleconfigs.HDFS.JOURNALNODE}
	    }
	    YARN {
		RESOURCEMANAGER: ${roleconfigs.YARN.RESOURCEMANAGER}
	    }
	    HIVE {
		HIVEMETASTORE: ${roleconfigs.HIVE.HIVEMETASTORE}
		HIVESERVER2: ${roleconfigs.HIVE.HIVESERVER2}

	    }
	}

	include file("services/azb.conf")
        count: 1
	instance.placementGroup: ${SPREAD_PLACEMENT_GROUP}
    }
    masterc { include file("services/masternnn.conf"), 
	      instance: ${instances.master} {
		  tags {
		      group: masters
		  }
		  dataDiskCount: 3
		  ebsVolumeCount: 3
		  ebsVolumeType: gp2
	      }

	      roles {
		  HDFS: [FAILOVERCONTROLLER, JOURNALNODE]
		  ZOOKEEPER: [SERVER]
		  YARN: [JOBHISTORY, RESOURCEMANAGER]
		  HIVE: [HIVESERVER2, HIVEMETASTORE]
	      }

	      configs {
		  ZOOKEEPER {
		      SERVER: ${roleconfigs.ZOOKEEPER.SERVER}
		  }
		  HDFS {
		      JOURNALNODE: ${roleconfigs.HDFS.JOURNALNODE}
		  }
		  YARN {
		      RESOURCEMANAGER: ${roleconfigs.YARN.RESOURCEMANAGER}
		  }
		  HIVE {
		      HIVEMETASTORE: ${roleconfigs.HIVE.HIVEMETASTORE}
		      HIVESERVER2: ${roleconfigs.HIVE.HIVESERVER2}
		  }
	      }

	      include file("services/azc.conf")
              count: 1
	      instance.placementGroup: ${SPREAD_PLACEMENT_GROUP}
	    }


    gateway { include file("services/gateway.conf"), include file("services/aza.conf")
              count: 1
	    }

    workersa { include file("services/worker.conf"), include file("services/aza.conf")
               count: 3
               minCount: 3
	       instance.placementGroup: ${CLUSTER_PLACEMENT_GROUPA}

	     }
    workersb { include file("services/worker.conf"), include file("services/azb.conf")
               count: 3
               minCount: 3
	       instance.placementGroup: ${CLUSTER_PLACEMENT_GROUPB}
	     }
    workersc { include file("services/worker.conf"), include file("services/azc.conf")
               count: 3
               minCount: 3
	       instance.placementGroup: ${CLUSTER_PLACEMENT_GROUPC}
	     }
}
